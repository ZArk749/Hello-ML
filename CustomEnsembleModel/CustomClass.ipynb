{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9ab8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "import joblib\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a47f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toy dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6020d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serious dataset\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_classes=3, n_samples=1000, n_features=6, n_informative=4, n_redundant=2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75a5398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hard dataset\n",
    "\n",
    "X, y = make_classification(n_classes=3, n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9476fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c1fcedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set 0 : 65.33 % of  0.0 class elements, as per seed\n",
      "set 1 : 61.45 % of  0.0 class elements, as per seed\n",
      "set 2 : 54.69 % of  1.0 class elements, as per seed\n",
      "set 3 : 83.48 % of  1.0 class elements, as per seed\n",
      "set 4 : 77.85 % of  0.0 class elements, as per seed\n",
      "set 5 : 85.73 % of  1.0 class elements, as per seed\n",
      "set 6 : 84.48 % of  0.0 class elements, as per seed\n",
      "set 7 : 84.36 % of  2.0 class elements, as per seed\n",
      "set 8 : 77.22 % of  2.0 class elements, as per seed\n",
      "set 9 : 82.98 % of  0.0 class elements, as per seed\n",
      "set 10 : 62.83 % of  0.0 class elements, as per seed\n",
      "set 11 : 17.65 % of  1.0 class elements, as per seed\n",
      "set 12 : 51.56 % of  1.0 class elements, as per seed\n",
      "set 13 : 58.45 % of  0.0 class elements, as per seed\n",
      "set 14 : 44.18 % of  2.0 class elements, as per seed\n",
      "set 15 : 71.59 % of  0.0 class elements, as per seed\n",
      "set 16 : 73.72 % of  2.0 class elements, as per seed\n",
      "set 17 : 71.34 % of  2.0 class elements, as per seed\n",
      "set 18 : 54.44 % of  0.0 class elements, as per seed\n",
      "set 19 : 38.42 % of  2.0 class elements, as per seed\n",
      "set 20 : 60.2 % of  1.0 class elements, as per seed\n",
      "set 21 : 31.16 % of  1.0 class elements, as per seed\n",
      "set 22 : 46.68 % of  1.0 class elements, as per seed\n",
      "set 23 : 35.29 % of  1.0 class elements, as per seed\n",
      "set 24 : 59.95 % of  2.0 class elements, as per seed\n",
      "set 25 : 35.79 % of  0.0 class elements, as per seed\n",
      "set 26 : 34.92 % of  2.0 class elements, as per seed\n",
      "set 27 : 38.42 % of  0.0 class elements, as per seed\n",
      "set 28 : 57.82 % of  1.0 class elements, as per seed\n",
      "set 29 : 51.44 % of  2.0 class elements, as per seed\n",
      "set 30 : 47.68 % of  1.0 class elements, as per seed\n",
      "set 31 : 49.44 % of  2.0 class elements, as per seed\n",
      "set 32 : 58.57 % of  1.0 class elements, as per seed\n",
      "set 33 : 32.17 % of  0.0 class elements, as per seed\n",
      "set 34 : 37.8 % of  1.0 class elements, as per seed\n",
      "set 35 : 31.54 % of  2.0 class elements, as per seed\n",
      "set 36 : 37.92 % of  1.0 class elements, as per seed\n",
      "set 37 : 26.28 % of  0.0 class elements, as per seed\n",
      "set 38 : 42.55 % of  1.0 class elements, as per seed\n",
      "set 39 : 38.67 % of  0.0 class elements, as per seed\n",
      "set 40 : 40.8 % of  1.0 class elements, as per seed\n",
      "set 41 : 30.79 % of  1.0 class elements, as per seed\n",
      "set 42 : 36.67 % of  0.0 class elements, as per seed\n",
      "set 43 : 39.05 % of  1.0 class elements, as per seed\n",
      "set 44 : 32.79 % of  2.0 class elements, as per seed\n",
      "set 45 : 33.17 % of  2.0 class elements, as per seed\n",
      "set 46 : 33.67 % of  2.0 class elements, as per seed\n",
      "set 47 : 30.41 % of  0.0 class elements, as per seed\n",
      "set 48 : 34.04 % of  2.0 class elements, as per seed\n",
      "set 49 : 36.67 % of  2.0 class elements, as per seed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from random import randint\n",
    "import random\n",
    "\n",
    "def custom_bootstrap(X, y, similarity_function, n_estimators):\n",
    "    \n",
    "    #Initializing list of random training sets\n",
    "    X_sets = []\n",
    "    y_sets = []\n",
    "    \n",
    "    for i in range(0,n_estimators):\n",
    "        instance_X = np.empty(np.shape(X))\n",
    "        instance_y = np.empty(np.shape(y))\n",
    "        \n",
    "        #creating seed for specific training set\n",
    "        seed = randint(0,len(X)-1)\n",
    "        \n",
    "        instance_X[0] = X[seed]\n",
    "        instance_y[0] = y[seed]       \n",
    "        \n",
    "        n_entries = 1\n",
    "        \n",
    "        #populating training set\n",
    "        \n",
    "        \n",
    "        if 1-(len(X_sets)+1)/n_estimators>0.8:\n",
    "            p_thresh = 0.8\n",
    "        else:\n",
    "            p_thresh = np.round(1-(len(X_sets)+1)/n_estimators,1)\n",
    "        \n",
    "        #print(\"populating set\", i, \"with probability acceptance\",p_thresh)\n",
    "        \n",
    "        \n",
    "        while n_entries < len(X):\n",
    "            \n",
    "            rand = randint(0,len(X)-1)\n",
    "            \n",
    "            if accept_entry(similarity_function[seed,rand],n_entries/(len(X)-1),p_thresh):\n",
    "                \n",
    "                instance_X[n_entries] = X[rand]\n",
    "                instance_y[n_entries] = y[rand]\n",
    "                n_entries+=1\n",
    "                \n",
    "        X_sets.append(instance_X)\n",
    "        y_sets.append(instance_y)     \n",
    "    \n",
    "    return X_sets,y_sets\n",
    "\n",
    "def accept_entry(similarity_score, p_ratio, prob_threshold = 0.7):\n",
    "    #p_ratio gradually shifts the decisional weight from similarity_score to randomness as the test set gets filled up.\n",
    "    #Specifically, at least least a small number of entries are accepted only through randomness (p_ratio=1)\n",
    "    #this way it's unlikely for the method to come up with sets with only one class (if not impossible, but I need to check)\n",
    "    \n",
    "    #It is also possible to modify the prob_threshold in order to get more or less random values, assuring more variance.\n",
    "    #I think this tweakability could prove useful to adapt the model to specific instances.\n",
    "    \n",
    "    #PLEASE NOTE THAT THIS CODE MAKES SENSE WITH NORMALIZED SIMILARITY SCORES\n",
    "    #NORMALIZATION FOR FUNCTION THAT DO NOT RETURN [0,1] VALUES STILL NEEDS TO BE IMPLEMENTED.\n",
    "    #It's not like it doesn't work, but prob calculations take the high road\n",
    "    \n",
    "    prob = similarity_score*(1-p_ratio)+ random.random()*p_ratio\n",
    "    if prob > prob_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def normalize(X):\n",
    "    #todo, in case the class needs to accept non-normalized similarity functions\n",
    "    return X\n",
    "\n",
    "\n",
    "#Test\n",
    "X_sets, y_sets = custom_bootstrap(X_train, y_train,pairwise_kernels(X_train, metric='rbf'),50)\n",
    "\n",
    "for i in range(len(X_sets)):\n",
    "    for j in range (0,len(X_sets[0][:,0])):\n",
    "        if j==0:\n",
    "            s = y_sets[i][j]\n",
    "            g=0\n",
    "        else:\n",
    "            if y_sets[i][j] == s:\n",
    "                g+=1\n",
    "    #used to measure the effects of p_thresh\n",
    "    print (\"set\",i,\":\",np.round(g/j*100,2),\"% of \",y_sets[i][j],\"class elements, as per seed\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c060de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEstimator(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_estimators,\n",
    "                base_estimator=DecisionTreeClassifier(),\n",
    "                similarity_function=pairwise_kernels(X, X, metric='rbf'),\n",
    "                debug = False):\n",
    "        \n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_estimator = base_estimator\n",
    "        self.similarity_function = similarity_function\n",
    "        self.debug=debug\n",
    "        \n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        self.n_classes = np.max(y)+1\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"Bootstrapping...\")\n",
    "        self.X_sets, self.y_sets = custom_bootstrap(X, y, self.similarity_function, self.n_estimators)\n",
    "        #dummy_value = self.base_estimator.fit(X,y)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"Fitting...\")\n",
    "        \n",
    "        for i in range(self.n_estimators):        \n",
    "            model = self.base_estimator.fit(self.X_sets[i], self.y_sets[i])\n",
    "            model_name = \"model\"+str(i)+\".pkl\"\n",
    "            joblib.dump(model, model_name)\n",
    "            \n",
    "            \n",
    "        #self.value_ = dummy_value\n",
    "        \n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Done!\")\n",
    "        #return self #For some reason this produces an error\n",
    "        \n",
    "            \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        \n",
    "        \n",
    "        out = np.zeros(len(X))\n",
    "        probs = self.predict_proba(X)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"Predict...\")\n",
    "        \n",
    "        for i in range (len(X)):\n",
    "            maximum = np.max(probs[i])\n",
    "            for j in range (self.n_classes):\n",
    "                if probs[i][j] == maximum:\n",
    "                    out[i] = j\n",
    "                    \n",
    "                \n",
    "        return out\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "             \n",
    "        out = np.zeros(shape=(len(X),self.n_classes))\n",
    "        \n",
    "        #compute similarity between instance and each training set\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"Computing models similarity to instance...\")\n",
    "        \n",
    "        Z=[]\n",
    "        for i in range (0,self.n_estimators):\n",
    "            Z.append(pairwise_kernels(X, self.X_sets[i], metric='rbf'))\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"Making predictions with each trained model...\")\n",
    "            \n",
    "        for i in range(0,len(X)): #for each sample to predict\n",
    "            #print(\"evaluating sample\",i)\n",
    "            \n",
    "            votes = []\n",
    "            sim_means = np.zeros(self.n_estimators)\n",
    "            \n",
    "            voting_power = np.zeros(self.n_estimators)\n",
    "            \n",
    "            for j in range (0,self.n_estimators): #for each estimator trained\n",
    "               \n",
    "                #load estimator model\n",
    "                model_name = \"model\"+str(j)+\".pkl\"\n",
    "                model = joblib.load(model_name)\n",
    "                \n",
    "                votes.append(model.predict_proba(X[i].reshape(1, -1)))#*np.mean(Z[j][i]) #not sure about using the mean\n",
    "                sim_means[j] = np.mean(Z[j][i])\n",
    "            \n",
    "                if self.debug:\n",
    "                    #print(\"votes:\",np.round(votes[j],2),\"similarity:\",np.round(sim_means[j],2))\n",
    "                    pass\n",
    "           \n",
    "                voting_power[j] = (100*sim_means[j]/np.sum(sim_means))/100\n",
    "                votes[j]=votes[j]*voting_power[j]\n",
    "        \n",
    "            \n",
    "            #print(\"voting power:\",voting_power)\n",
    "            out[i]=(np.sum(votes,axis=0))\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"Done!\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2544b9e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping...\n",
      "Fitting...\n",
      "Done!\n",
      "Computing models similarity to instance...\n",
      "Making predictions with each trained model...\n",
      "Done!\n",
      "Predict...\n",
      "Accuracy = 0.8\n"
     ]
    }
   ],
   "source": [
    "# Test instance \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "custom_clf = CustomEstimator(n_estimators=50,\n",
    "                             base_estimator=DecisionTreeClassifier(),\n",
    "                             similarity_function=pairwise_kernels(X, metric='rbf'),debug=True)\n",
    "\n",
    "custom_clf.fit(X_train, y_train)\n",
    "y_pred=custom_clf.predict(X_test)\n",
    "print(\"Accuracy =\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Commenting this here as I won't do it again: \n",
    "#tested 7/18/22 version with make_classification with 4 classes, 500 estimators\n",
    "#Accuracy score ~0.635\n",
    "#Somehow each estimator predicts the same thing for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86be773b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores:\n",
      "RandomForestClassifier : 0.8\n",
      "SVC : 0.86\n",
      "DecisionTreeClassifier : 0.78\n",
      "BaggingClassifier : 0.795\n",
      "Bootstrapping...\n",
      "Fitting...\n",
      "Done!\n",
      "Computing models similarity to instance...\n",
      "Making predictions with each trained model...\n",
      "Done!\n",
      "Predict...\n",
      "CustomEstimator : 0.785\n"
     ]
    }
   ],
   "source": [
    "# Comparative tests\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=50, max_leaf_nodes=16, n_jobs=-1)\n",
    "\n",
    "svc_clf = SVC(probability=True)\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=16)\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, max_samples=100, bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "custom_clf = CustomEstimator(n_estimators=50,base_estimator=DecisionTreeClassifier())\n",
    "\n",
    "print(\"Accuracy scores:\")\n",
    "for clf in (rnd_clf, svc_clf, tree_clf, bag_clf, custom_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(clf.__class__.__name__,\":\",metrics.accuracy_score(y_test, y_pred))\n",
    "#     results, names = list(), list()\n",
    "#     scores = evaluate_model(clf, X, y)\n",
    "#     results.append(scores)\n",
    "#     names.append(clf.__class__.__name__)\n",
    "#     print('%s %.3f (%.3f)' % (\"Cross validation score:\", np.mean(scores), np.std(scores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7ab5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
