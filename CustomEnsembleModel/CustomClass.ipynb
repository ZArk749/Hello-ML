{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9ab8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "import joblib\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75a5398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hard dataset\n",
    "\n",
    "X, y = make_classification(n_classes=3, n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53a47f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toy dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe4e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = make_classification(n_classes=3, n_samples=500, n_features=6, n_informative=5, n_redundant=1, random_state=2)\n",
    "c, d = make_classification(n_classes=3, n_samples=500, n_features=6, n_informative=3, n_redundant=3, random_state=44)\n",
    "X = np.concatenate((a,c))\n",
    "y= np.concatenate((b,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6020d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serious dataset\n",
    "\n",
    "X, y = make_classification(n_classes=3, n_samples=1000, n_features=6, n_informative=4, n_redundant=2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9476fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1168304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def _validate(metric, bootstrap_method):\n",
    "    \n",
    "    if bootstrap_method is not None:\n",
    "        if bootstrap_method not in (\"rejection\",\"randomchoice\"):\n",
    "            raise ValueError(\n",
    "                'Invalid preset \"%s\" for bootstrap_method'\n",
    "                % bootstrap_method\n",
    "                )\n",
    "    \n",
    "    if metric is not None:\n",
    "        if metric not in (\"rbf\", \"laplacian\"):\n",
    "            raise ValueError(\n",
    "                'Invalid preset \"%s\" for kernel metric'\n",
    "                % metric\n",
    "                )\n",
    "            \n",
    "def _pickDiverseSeed(sim_matrix,seed=None):\n",
    "    #creating seed for specific training set, choosing by inverse similarity to previous set\n",
    "    if seed is None:\n",
    "            return randint(0,len(sim_matrix)-1)\n",
    "    else:\n",
    "        #questo è probabilmente il codice più cringe che io abbia mai scritto,\n",
    "        #ma non riesco a fargli ritornare un int se non specificando l'indice\n",
    "\n",
    "        sim_n = 1-sim_matrix[seed]\n",
    "\n",
    "        candidate = np.random.choice(np.arange(len(sim_matrix)), size=1, replace=True,\n",
    "                                     p=sim_n/np.sum(sim_n))[0]\n",
    "\n",
    "        return candidate\n",
    "    \n",
    "def _pickDiverseSeed2(sim_matrix,seed): #Randomly picks from all used seeds, does not improve much\n",
    "    l=len(seed)\n",
    "    if len(seed) == 0:\n",
    "        return randint(0,len(sim_matrix)-1)\n",
    "    else:\n",
    "        \n",
    "        candidates = np.zeros(l)\n",
    "        \n",
    "        for i in range(0,len(seed)):\n",
    "\n",
    "            sim_n = 1-sim_matrix[seed[i]]\n",
    "\n",
    "            candidates[i] = np.random.choice(np.arange(len(sim_matrix)), size=1, replace=True, \n",
    "                                             p=sim_n/np.sum(sim_n))[0]\n",
    "        \n",
    "\n",
    "        return int(np.random.choice(candidates, size=1, replace=True)[0])\n",
    "    \n",
    "def _parallel_fit(n_estimators, base_estimator, X, y, i):\n",
    "       \n",
    "            model = base_estimator.fit(X[i], y[i])\n",
    "            model_name = \"model\"+str(i)+\".pkl\"\n",
    "            joblib.dump(model, model_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a11a356e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _randomchoice_bootstrap(X, y, similarity_metric, n_estimators, verbose=0):\n",
    "    #Initializing list of random training sets\n",
    "    X_sets = []\n",
    "    y_sets = []\n",
    "    G = pairwise_kernels(X, metric=similarity_metric)\n",
    "    \n",
    "    seed=None\n",
    "    \n",
    "    for i in range(0,n_estimators):\n",
    "        instance_X = np.empty(np.shape(X))\n",
    "        instance_y = np.empty(np.shape(y))\n",
    "        \n",
    "        #creating seed for specific training set\n",
    "        \n",
    "        if verbose>0:\n",
    "            oldseed=seed\n",
    "        \n",
    "        seed = _pickDiverseSeed(G,seed)\n",
    "        \n",
    "        if verbose>0 and oldseed is not None:\n",
    "            print(\"Chose new seed\",seed,\"based on previous one:\",oldseed,\"w/ similarity:\",G[seed][oldseed])\n",
    "                  \n",
    "\n",
    "        instance_X[0] = X[seed]\n",
    "        instance_y[0] = y[seed]\n",
    "        \n",
    "        pool = np.arange(np.shape(X)[0])\n",
    "        pool_prob = np.zeros(np.shape(X)[0])\n",
    "        pool_tot = np.sum(G[seed])\n",
    "        \n",
    "        \n",
    "        for i in range(np.shape(X)[0]):\n",
    "            pool_prob[i] = G[seed][i]/pool_tot\n",
    "        \n",
    "        if False:  #adds noise, does not increase accuracy\n",
    "            for i in range(0,int(np.shape(X)[0]/2)):\n",
    "                offset = 0.3 #random.uniform(0, 1)\n",
    "                \n",
    "                index = randint(0,np.shape(X)[0]-1)\n",
    "                while pool_prob[index] + offset > 1: \n",
    "                    index = randint(0,np.shape(X)[0]-1)\n",
    "                pool_prob[index] += offset\n",
    "                \n",
    "                index = randint(0,np.shape(X)[0])-1\n",
    "                while pool_prob[index] - offset < 0: \n",
    "                    index = randint(0,np.shape(X)[0]-1)\n",
    "                pool_prob[index] -= offset\n",
    "                \n",
    "                \n",
    "#         print(pool_prob)\n",
    "        \n",
    "#         print(G[seed])\n",
    "        \n",
    "        pick = np.random.choice(pool, size=(len(pool))-1, replace=True, p=pool_prob)\n",
    "        \n",
    "        for i in range(len(pick)):\n",
    "            instance_X[i+1] = X[pool[i]]\n",
    "            instance_y[i+1] = y[pool[i]]\n",
    "        \n",
    "        X_sets.append(instance_X)\n",
    "        y_sets.append(instance_y)\n",
    "    \n",
    "    return X_sets,y_sets\n",
    "\n",
    "#Test\n",
    "X_sets,y_sets = _randomchoice_bootstrap(X_train,y_train,\"rbf\",n_estimators=50,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bdf1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "def _rejection_bootstrap(X, y, similarity_metric, n_estimators, n_jobs):\n",
    "    \n",
    "    #Initializing list of random training sets\n",
    "    X_sets = []\n",
    "    y_sets = []\n",
    "    G = pairwise_kernels(X, metric=similarity_metric)\n",
    "    \n",
    "    seed=None\n",
    "\n",
    "# CURRENTLY RETURNS ERROR IF n_jobs!=1\n",
    "#     Parallel(n_jobs=n_jobs)(delayed(_parallel_rejection_bootstrap)(G,seed,n_estimators,X,y,X_sets,y_sets) \n",
    "#                                   for i in range(0,n_estimators))\n",
    "    \n",
    "    for i in range(0,n_estimators):  #for each estimator  \n",
    "            _parallel_rejection_bootstrap(G,seed,n_estimators,X,y,X_sets,y_sets)   \n",
    "    \n",
    "    return X_sets,y_sets\n",
    "\n",
    "def _parallel_rejection_bootstrap(G,seed,n_estimators,X,y,X_sets,y_sets):\n",
    "    instance_X = np.empty(np.shape(X))\n",
    "    instance_y = np.empty(np.shape(y))\n",
    "\n",
    "    seed = _pickDiverseSeed(G,seed)\n",
    "\n",
    "    instance_X[0] = X[seed]\n",
    "    instance_y[0] = y[seed]       \n",
    "\n",
    "    n_entries = 1\n",
    "\n",
    "    #populating training set\n",
    "\n",
    "\n",
    "    if 1-(len(X_sets)+1)/n_estimators>0.8:\n",
    "        p_thresh = 0.8\n",
    "    else:\n",
    "        p_thresh = np.round(1-(len(X_sets)+1)/n_estimators,1)\n",
    "\n",
    "    #print(\"populating set\", i, \"with probability acceptance\",p_thresh)\n",
    "\n",
    "\n",
    "    while n_entries < len(X): #until the pool isn't filled up\n",
    "\n",
    "        rand = randint(0,len(X)-1)\n",
    "\n",
    "        if _accept_entry(G[seed,rand],n_entries/(len(X)-1),p_thresh):\n",
    "\n",
    "            instance_X[n_entries] = X[rand]\n",
    "            instance_y[n_entries] = y[rand]\n",
    "            n_entries+=1\n",
    "\n",
    "    X_sets.append(instance_X)\n",
    "    y_sets.append(instance_y)\n",
    "\n",
    "def _accept_entry(similarity_score, p_ratio, prob_threshold = 0.7):\n",
    "    #p_ratio gradually shifts the decisional weight from similarity_score to randomness as the test set gets filled up.\n",
    "    #Specifically, at least least a small number of entries are accepted only through randomness (p_ratio=1)\n",
    "    #this way it's unlikely for the method to come up with sets with only one class (if not impossible, but I need to check)\n",
    "    \n",
    "    #It is also possible to modify the prob_threshold in order to get more or less random values, assuring more variance.\n",
    "    #I think this tweakability could prove useful to adapt the model to specific instances.\n",
    "    \n",
    "    #PLEASE NOTE THAT THIS CODE MAKES SENSE WITH NORMALIZED SIMILARITY SCORES\n",
    "    #NORMALIZATION FOR FUNCTION THAT DO NOT RETURN [0,1] VALUES STILL NEEDS TO BE IMPLEMENTED.\n",
    "    #It's not like it doesn't work, but prob calculations take the high road\n",
    "    \n",
    "    prob = similarity_score*(1-p_ratio)+ random.random()*p_ratio\n",
    "    if prob > prob_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "#Test\n",
    "X_sets, y_sets = _rejection_bootstrap(X_train, y_train,'rbf',50,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c060de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_estimators : int, default=50\n",
    "        The number of models to train.\n",
    "        \n",
    "    base_estimator : estimator, default=DecisionTreeClassifier()\n",
    "        The estimator fitted on  each bootstrapped set.     \n",
    "        \n",
    "    n_jobs : int, default=4\n",
    "        Number of parallel jobs during fitting.\n",
    "        \n",
    "    similarity_metric : {\"rbf\", \"laplacian\", \"cosine\"}, string, default=\"rbf\"\n",
    "        The metric used for pairwise_kernels().\n",
    "        \n",
    "    bootstrap_method={\"rejection\", \"randomchoice\"}, string, default=TODO\n",
    "        The bootstrap method of choice.\n",
    "    \n",
    "    verbose : int, default = 0\n",
    "        Controls verbosity during fitting and predicting, 0 being none and 3 being the most detailed. \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class CustomEstimator(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, n_estimators=50,\n",
    "                base_estimator=DecisionTreeClassifier(),\n",
    "                n_jobs=1,\n",
    "                bootstrap_method=\"rejection\",\n",
    "                similarity_metric=\"rbf\",\n",
    "                verbose = 0):\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.n_estimators = n_estimators \n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_jobs = n_jobs\n",
    "        self.bootstrap_method = bootstrap_method\n",
    "        self.similarity_metric = similarity_metric\n",
    "        self.verbose = verbose \n",
    "        \n",
    "        _validate(self.similarity_metric, self.bootstrap_method)\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        self.n_classes_= np.max(y)+1\n",
    "        \n",
    "        if self.verbose > 0:\n",
    "            print(\"Bootstrapping...\")\n",
    "            \n",
    "        if self.bootstrap_method==\"rejection\":  \n",
    "            self.X_sets, self.y_sets = _rejection_bootstrap(X, y, self.similarity_metric, self.n_estimators, self.n_jobs)\n",
    "        elif self.bootstrap_method==\"randomchoice\": \n",
    "            self.X_sets, self.y_sets = _randomchoice_bootstrap(X, y, self.similarity_metric, self.n_estimators)\n",
    "        \n",
    "        \n",
    "        if self.verbose > 0:\n",
    "            print(\"Fitting...\")\n",
    "        \n",
    "        Parallel(n_jobs=self.n_jobs)(delayed(_parallel_fit)(self.n_estimators, self.base_estimator, self.X_sets, self.y_sets, i) \n",
    "                           for i in range(0,self.n_estimators))\n",
    "        \n",
    "#         for i in range(self.n_estimators):\n",
    "#             _parallel_fit(self.n_estimators, self.base_estimator, self.X_sets, self.y_sets, i)\n",
    "            \n",
    "        \n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(\"Done!\")\n",
    "        return self\n",
    "                \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        predicted_probabilitiy = self.predict_proba(X)\n",
    "        return np.argmax(predicted_probabilitiy, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "            \n",
    "        if self.verbose > 0:\n",
    "            print(\"Predicting...\")\n",
    "            \n",
    "        out = np.zeros(shape=(len(X),self.n_classes_))\n",
    "        \n",
    "        #compute similarity between instance and each training set\n",
    "        \n",
    "        if self.verbose > 0:\n",
    "            print(\"Computing models similarity to instance...\")\n",
    "        \n",
    "        Z=[]\n",
    "        for j in range (0,self.n_estimators):\n",
    "            Z.append(pairwise_kernels(X, self.X_sets[j], metric=self.similarity_metric))\n",
    "#             if self.verbose > 1:\n",
    "#                 print(\"Similarity matrix for instance to model\",j,\":\",Z[j])\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.verbose > 0:\n",
    "            print(\"Making predictions with each trained model...\")\n",
    "            \n",
    "          \n",
    "        for i in range(0,len(X)): #for each sample to predict\n",
    "            if self.verbose > 1:  \n",
    "                print(\"\\n--- EVALUATING SAMPLE\",i,\" ---\")\n",
    "            \n",
    "            votes = []\n",
    "            votes_weighted = []\n",
    "            sim_means = np.zeros(self.n_estimators)\n",
    "            \n",
    "            voting_power = np.zeros(self.n_estimators)\n",
    "            \n",
    "            for j in range (0,self.n_estimators): #for each estimator trained\n",
    "               \n",
    "                #load estimator model\n",
    "                model_name = \"model\"+str(j)+\".pkl\"\n",
    "                model = joblib.load(model_name)\n",
    "                \n",
    "                votes.append(model.predict_proba(X[i].reshape(1, -1))) \n",
    "                sim_means[j] = np.mean(Z[j][i])\n",
    "                \n",
    "                \n",
    "            for j in range (0,self.n_estimators):\n",
    "                     \n",
    "                voting_power[j] = (100*sim_means[j]/np.sum(sim_means))/100\n",
    "                votes_weighted.append(votes[j]*voting_power[j])\n",
    "                \n",
    "                if self.verbose > 2:\n",
    "                    print(\"model \",j,\"votes:\",np.round(votes[j],4),\"having ~\",np.round(sim_means[j],4),\n",
    "                          \"similarity (voting power:\",np.round(voting_power[j],4),\")\")\n",
    "                \n",
    "            out[i]=(np.sum(votes_weighted,axis=0))\n",
    "            if self.verbose > 1:\n",
    "                print(\"FINAL PREDICTION:\",out[i],)\n",
    "            \n",
    "        \n",
    "        if self.verbose > 0:\n",
    "            print(\"Done!\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74215001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping...\n",
      "Fitting...\n",
      "Done!\n",
      "CPU times: total: 516 ms\n",
      "Wall time: 3.06 s\n",
      "Predicting...\n",
      "Computing models similarity to instance...\n",
      "Making predictions with each trained model...\n",
      "Done!\n",
      "Accuracy = 0.805\n"
     ]
    }
   ],
   "source": [
    "# Test instance \n",
    "\n",
    "clf = CustomEstimator(n_estimators=50, base_estimator=DecisionTreeClassifier(), n_jobs=4,\n",
    "                             similarity_metric='rbf', bootstrap_method=\"randomchoice\",verbose=1)\n",
    "\n",
    "%time clf.fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(\"Accuracy =\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ca0c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.2 s\n",
      "Wall time: 4.25 s\n",
      "CPU times: total: 5.42 s\n",
      "Wall time: 5.49 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CustomEstimator(n_jobs=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CustomEstimator</label><div class=\"sk-toggleable__content\"><pre>CustomEstimator(n_jobs=4)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "CustomEstimator(n_jobs=4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test parallelizzazione\n",
    "clf1 = CustomEstimator(n_estimators=50, base_estimator=DecisionTreeClassifier(), n_jobs=1,\n",
    "                             similarity_metric='rbf', bootstrap_method=\"rejection\")\n",
    "\n",
    "clf2 = CustomEstimator(n_estimators=50, base_estimator=DecisionTreeClassifier(), n_jobs=4,\n",
    "                             similarity_metric='rbf', bootstrap_method=\"rejection\")\n",
    "\n",
    "%time clf1.fit(X_train, y_train)\n",
    "%time clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "533df5ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping...\n",
      "Fitting...\n",
      "Done!\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "Predicting...\n",
      "Computing models similarity to instance...\n",
      "Making predictions with each trained model...\n",
      "Done!\n",
      "[[0.09436731 0.08384111 0.82179158]\n",
      " [1.         0.         0.        ]\n",
      " [0.71435899 0.23328952 0.05235148]\n",
      " [0.83030452 0.15229973 0.01739575]\n",
      " [0.         0.04166825 0.95833175]\n",
      " [0.74229939 0.24463192 0.0130687 ]\n",
      " [0.         0.21926325 0.78073675]\n",
      " [0.32874138 0.67125862 0.        ]\n",
      " [0.         0.11102548 0.88897452]\n",
      " [0.44645408 0.46534039 0.08820553]\n",
      " [0.30997527 0.67586008 0.01416465]\n",
      " [0.59030085 0.28377155 0.1259276 ]\n",
      " [0.         0.09528959 0.90471041]\n",
      " [0.03957643 0.11096252 0.84946104]\n",
      " [0.96415187 0.03584813 0.        ]\n",
      " [0.98891575 0.01108425 0.        ]\n",
      " [0.06095154 0.9261053  0.01294315]\n",
      " [0.00359595 0.94199637 0.05440768]\n",
      " [0.81466627 0.09926757 0.08606616]\n",
      " [0.25178128 0.56049023 0.18772849]\n",
      " [0.59978088 0.0505659  0.34965322]\n",
      " [0.23111659 0.63989968 0.12898373]\n",
      " [0.4826243  0.08231261 0.43506309]\n",
      " [0.76595843 0.13489403 0.09914754]\n",
      " [0.1480006  0.80487287 0.04712652]\n",
      " [0.03331391 0.91592711 0.05075898]\n",
      " [0.03048993 0.85551495 0.11399511]\n",
      " [0.03639654 0.96360346 0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [0.02938825 0.97061175 0.        ]\n",
      " [0.87396676 0.02152597 0.10450727]\n",
      " [0.73230508 0.17857688 0.08911804]\n",
      " [0.08409763 0.54899894 0.36690342]\n",
      " [0.05066645 0.82504319 0.12429037]\n",
      " [0.57351709 0.12539941 0.3010835 ]\n",
      " [0.2349753  0.05134024 0.71368445]\n",
      " [0.72438559 0.03675009 0.23886432]\n",
      " [0.02745345 0.88743158 0.08511497]\n",
      " [0.         0.01752232 0.98247768]\n",
      " [0.03255435 0.71108668 0.25635897]\n",
      " [0.00554617 0.99445383 0.        ]\n",
      " [0.86191833 0.13808167 0.        ]\n",
      " [0.27586453 0.65349202 0.07064345]\n",
      " [0.01274843 0.02894614 0.95830543]\n",
      " [0.         0.55397773 0.44602227]\n",
      " [0.18459879 0.79457677 0.02082445]\n",
      " [0.09277262 0.01621738 0.89101   ]\n",
      " [0.05000729 0.53533342 0.41465929]\n",
      " [0.77155065 0.11953742 0.10891193]\n",
      " [0.90744658 0.00258814 0.08996528]\n",
      " [0.57656758 0.1413971  0.28203532]\n",
      " [0.94860571 0.03498648 0.01640781]\n",
      " [0.99394506 0.         0.00605494]\n",
      " [0.86158203 0.04450849 0.09390949]\n",
      " [0.05885789 0.11677409 0.82436802]\n",
      " [0.69137156 0.29251635 0.01611209]\n",
      " [0.70363582 0.09709107 0.19927311]\n",
      " [0.50770067 0.34101882 0.15128051]\n",
      " [0.99462175 0.         0.00537825]\n",
      " [0.         0.42442591 0.57557409]\n",
      " [0.87873787 0.12126213 0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [0.72775691 0.18069597 0.09154713]\n",
      " [0.85387701 0.14612299 0.        ]\n",
      " [0.         0.02317718 0.97682282]\n",
      " [0.11513544 0.62002207 0.26484249]\n",
      " [0.01405689 0.07686144 0.90908167]\n",
      " [0.00551713 0.7356594  0.25882347]\n",
      " [0.         0.37735256 0.62264744]\n",
      " [0.03888686 0.93572661 0.02538653]\n",
      " [0.01573403 0.8900925  0.09417347]\n",
      " [0.03574346 0.54709363 0.41716291]\n",
      " [0.58044187 0.05915906 0.36039907]\n",
      " [0.90132902 0.09867098 0.        ]\n",
      " [0.         0.26726084 0.73273916]\n",
      " [0.05959667 0.64374659 0.29665674]\n",
      " [0.12577948 0.87422052 0.        ]\n",
      " [0.0086558  0.51721117 0.47413303]\n",
      " [0.06504142 0.17464369 0.76031489]\n",
      " [0.70897961 0.09496478 0.19605561]\n",
      " [0.07340135 0.33275306 0.59384559]\n",
      " [0.11417363 0.70144893 0.18437744]\n",
      " [0.         0.         1.        ]\n",
      " [0.08620798 0.87704163 0.03675039]\n",
      " [0.89422572 0.08153765 0.02423663]\n",
      " [0.03731277 0.21335583 0.7493314 ]\n",
      " [0.14102154 0.01041142 0.84856704]\n",
      " [0.63276653 0.28752436 0.07970911]\n",
      " [0.15819959 0.84180041 0.        ]\n",
      " [0.         0.14037389 0.85962611]\n",
      " [1.         0.         0.        ]\n",
      " [0.0152579  0.95621999 0.02852211]\n",
      " [0.05071021 0.92064943 0.02864036]\n",
      " [0.         0.01906454 0.98093546]\n",
      " [0.         0.33473447 0.66526553]\n",
      " [0.1462699  0.16791533 0.68581476]\n",
      " [0.69354772 0.20014292 0.10630936]\n",
      " [0.02023037 0.90780073 0.0719689 ]\n",
      " [0.26769648 0.04413394 0.68816958]\n",
      " [1.         0.         0.        ]\n",
      " [0.         0.1418531  0.8581469 ]\n",
      " [0.09167643 0.86803031 0.04029326]\n",
      " [0.84805446 0.15194554 0.        ]\n",
      " [0.06752544 0.02130293 0.91117162]\n",
      " [0.00300895 0.03037544 0.96661561]\n",
      " [0.77550915 0.22449085 0.        ]\n",
      " [0.0404676  0.9595324  0.        ]\n",
      " [0.05656765 0.1489462  0.79448615]\n",
      " [0.02162642 0.88587263 0.09250095]\n",
      " [0.92937652 0.05967696 0.01094652]\n",
      " [0.01518338 0.0029847  0.98183192]\n",
      " [0.         0.19291252 0.80708748]\n",
      " [0.01701361 0.01784523 0.96514116]\n",
      " [0.         0.85727718 0.14272282]\n",
      " [0.0126174  0.89839534 0.08898725]\n",
      " [0.         0.03179465 0.96820535]\n",
      " [0.00533881 0.72566767 0.26899352]\n",
      " [0.73384521 0.22253488 0.04361991]\n",
      " [0.00788373 0.09438793 0.89772834]\n",
      " [0.05577912 0.94422088 0.        ]\n",
      " [0.00195573 0.04161205 0.95643223]\n",
      " [0.67563082 0.09416204 0.23020714]\n",
      " [0.03704707 0.94361354 0.01933939]\n",
      " [0.0434794  0.91608767 0.04043293]\n",
      " [0.         0.54016196 0.45983804]\n",
      " [0.66891688 0.22666641 0.10441671]\n",
      " [0.76184723 0.23815277 0.        ]\n",
      " [0.04575481 0.39836754 0.55587766]\n",
      " [0.92905463 0.01230141 0.05864396]\n",
      " [0.04183023 0.41564464 0.54252512]\n",
      " [0.01812245 0.22002711 0.76185044]\n",
      " [0.04884124 0.33503608 0.61612268]\n",
      " [0.32864465 0.14438155 0.5269738 ]\n",
      " [0.03470329 0.75547791 0.20981881]\n",
      " [0.96118707 0.01372332 0.02508962]\n",
      " [0.46281322 0.50475956 0.03242723]\n",
      " [0.         0.23561081 0.76438919]\n",
      " [0.03358277 0.70176083 0.2646564 ]\n",
      " [0.96902633 0.         0.03097367]\n",
      " [0.7976007  0.         0.2023993 ]\n",
      " [0.01382945 0.11056132 0.87560923]\n",
      " [0.15213391 0.5895206  0.25834549]\n",
      " [0.59465144 0.18468429 0.22066427]\n",
      " [0.06391919 0.89294554 0.04313527]\n",
      " [0.18439227 0.81560773 0.        ]\n",
      " [0.72894764 0.24871217 0.02234019]\n",
      " [0.20592563 0.76851261 0.02556176]\n",
      " [0.01649176 0.98350824 0.        ]\n",
      " [0.08271069 0.06280508 0.85448423]\n",
      " [0.3025687  0.6974313  0.        ]\n",
      " [0.16352502 0.67415515 0.16231983]\n",
      " [0.02067205 0.04106139 0.93826656]\n",
      " [0.62236488 0.03352928 0.34410585]\n",
      " [0.57992811 0.29319005 0.12688184]\n",
      " [0.09484136 0.11090527 0.79425337]\n",
      " [0.04695933 0.47097342 0.48206725]\n",
      " [0.         0.26930126 0.73069874]\n",
      " [0.23056135 0.68353022 0.08590842]\n",
      " [0.02510315 0.97489685 0.        ]\n",
      " [0.75084186 0.14490765 0.1042505 ]\n",
      " [0.0785596  0.59624577 0.32519463]\n",
      " [0.95168852 0.02734509 0.02096638]\n",
      " [0.91003218 0.         0.08996782]\n",
      " [0.00550513 0.80099496 0.19349992]\n",
      " [0.         0.         1.        ]\n",
      " [0.89217495 0.01468153 0.09314352]\n",
      " [0.03925239 0.02080785 0.93993976]\n",
      " [0.64387042 0.35612958 0.        ]\n",
      " [0.06631678 0.66037358 0.27330964]\n",
      " [0.13264542 0.73914028 0.1282143 ]\n",
      " [0.67783042 0.31351354 0.00865604]\n",
      " [0.01329217 0.97660315 0.01010468]\n",
      " [0.0489269  0.49829865 0.45277445]\n",
      " [0.05964755 0.93024454 0.01010791]\n",
      " [0.84445329 0.06690732 0.08863939]\n",
      " [0.12452232 0.00922962 0.86624806]\n",
      " [0.18212813 0.07080683 0.74706504]\n",
      " [0.76740861 0.20807596 0.02451543]\n",
      " [0.         0.06082888 0.93917112]\n",
      " [0.         0.69623282 0.30376718]\n",
      " [0.51740586 0.31947322 0.16312092]\n",
      " [0.68275986 0.04436462 0.27287551]\n",
      " [0.         0.00717288 0.99282712]\n",
      " [0.         0.01193323 0.98806677]\n",
      " [0.         1.         0.        ]\n",
      " [0.00677089 0.41076383 0.58246528]\n",
      " [0.         0.02039943 0.97960057]\n",
      " [0.         0.08207061 0.91792939]\n",
      " [0.94195837 0.0209193  0.03712233]\n",
      " [1.         0.         0.        ]\n",
      " [0.32955723 0.36215959 0.30828317]\n",
      " [0.10151168 0.75064111 0.1478472 ]\n",
      " [0.51947476 0.1887765  0.29174874]\n",
      " [0.04926013 0.1697643  0.78097556]\n",
      " [0.         0.85757138 0.14242862]\n",
      " [0.         0.04086188 0.95913812]\n",
      " [0.08879504 0.52518633 0.38601862]\n",
      " [0.         0.02735348 0.97264652]\n",
      " [0.         0.06735227 0.93264773]\n",
      " [0.35607816 0.39661321 0.24730863]]\n",
      "[[0.14823079 0.13414486 0.71762436]\n",
      " [0.66395787 0.21334587 0.12269625]\n",
      " [0.744985   0.21259965 0.04241535]\n",
      " [0.77723264 0.18906212 0.03370524]\n",
      " [0.03450334 0.15829322 0.80720344]\n",
      " [0.58432767 0.25630459 0.15936774]\n",
      " [0.02598909 0.17120737 0.80280354]\n",
      " [0.43045558 0.53924159 0.03030283]\n",
      " [0.02425516 0.16925387 0.80649097]\n",
      " [0.47162792 0.43351102 0.09486107]\n",
      " [0.2437625  0.66638659 0.08985091]\n",
      " [0.59434097 0.33026563 0.0753934 ]\n",
      " [0.02938647 0.12048479 0.85012874]\n",
      " [0.02984919 0.12577994 0.84437087]\n",
      " [0.79970291 0.16018623 0.04011086]\n",
      " [0.89180357 0.09328977 0.01490666]\n",
      " [0.08450441 0.85351413 0.06198146]\n",
      " [0.15374856 0.6910014  0.15525004]\n",
      " [0.73184751 0.19517565 0.07297684]\n",
      " [0.22281255 0.47850072 0.29868673]\n",
      " [0.4054336  0.25496349 0.33960291]\n",
      " [0.1874414  0.57365807 0.23890053]\n",
      " [0.4575546  0.14025287 0.40219253]\n",
      " [0.65577026 0.21416171 0.13006802]\n",
      " [0.08473168 0.84965049 0.06561783]\n",
      " [0.06518808 0.61799322 0.3168187 ]\n",
      " [0.12506923 0.57367008 0.30126069]\n",
      " [0.08583125 0.85610033 0.05806842]\n",
      " [0.8558535  0.12471437 0.01943213]\n",
      " [0.09393136 0.84820605 0.05786259]\n",
      " [0.81347999 0.09366984 0.09285017]\n",
      " [0.59167334 0.33107438 0.07725228]\n",
      " [0.10350392 0.67009984 0.22639624]\n",
      " [0.2092431  0.63359105 0.15716585]\n",
      " [0.61503888 0.26712893 0.1178322 ]\n",
      " [0.18337465 0.20675312 0.60987223]\n",
      " [0.70639701 0.18134429 0.1122587 ]\n",
      " [0.20301915 0.69609399 0.10088686]\n",
      " [0.03160565 0.12682143 0.84157292]\n",
      " [0.18526632 0.51546414 0.29926954]\n",
      " [0.08871433 0.8570868  0.05419887]\n",
      " [0.68632726 0.25069133 0.06298142]\n",
      " [0.38134876 0.47370509 0.14494615]\n",
      " [0.05705557 0.16663108 0.77631334]\n",
      " [0.11692061 0.66147041 0.22160898]\n",
      " [0.13834284 0.7173667  0.14429045]\n",
      " [0.15891583 0.11130721 0.72977696]\n",
      " [0.08457944 0.46272814 0.45269242]\n",
      " [0.75683089 0.11096562 0.13220349]\n",
      " [0.71371796 0.1732725  0.11300955]\n",
      " [0.74683513 0.10975929 0.14340558]\n",
      " [0.83326597 0.13350207 0.03323196]\n",
      " [0.91391723 0.05214199 0.03394078]\n",
      " [0.66757079 0.26008062 0.07234859]\n",
      " [0.05054273 0.14075443 0.80870283]\n",
      " [0.6849104  0.25445032 0.06063928]\n",
      " [0.55369403 0.15613139 0.29017458]\n",
      " [0.25784662 0.48207784 0.26007554]\n",
      " [0.88513485 0.08072295 0.0341422 ]\n",
      " [0.03821386 0.20774932 0.75403682]\n",
      " [0.68764747 0.23615723 0.0761953 ]\n",
      " [0.89410358 0.05939014 0.04650628]\n",
      " [0.33149018 0.41822792 0.2502819 ]\n",
      " [0.86245715 0.11810088 0.01944197]\n",
      " [0.04935056 0.14424577 0.80640366]\n",
      " [0.11349218 0.3130749  0.57343292]\n",
      " [0.05194732 0.18808167 0.75997101]\n",
      " [0.15187803 0.64723145 0.20089051]\n",
      " [0.10172493 0.24543337 0.6528417 ]\n",
      " [0.11224123 0.83497397 0.0527848 ]\n",
      " [0.26042866 0.603694   0.13587734]\n",
      " [0.11883803 0.48074695 0.40041501]\n",
      " [0.48923917 0.13272905 0.37803178]\n",
      " [0.70688578 0.21007728 0.08303693]\n",
      " [0.04175253 0.11934262 0.83890485]\n",
      " [0.05042809 0.58679125 0.36278066]\n",
      " [0.14867274 0.60552808 0.24579918]\n",
      " [0.06089767 0.30968136 0.62942097]\n",
      " [0.28143662 0.48725016 0.23131322]\n",
      " [0.700396   0.09767407 0.20192993]\n",
      " [0.3751434  0.24342701 0.38142959]\n",
      " [0.21629795 0.5322611  0.25144095]\n",
      " [0.0294976  0.14777991 0.82272249]\n",
      " [0.24771729 0.56818103 0.18410168]\n",
      " [0.55217259 0.32606393 0.12176348]\n",
      " [0.03688289 0.17802458 0.78509253]\n",
      " [0.24425457 0.10497092 0.65077451]\n",
      " [0.56938901 0.36190414 0.06870686]\n",
      " [0.1477338  0.80764975 0.04461645]\n",
      " [0.05549247 0.20655899 0.73794855]\n",
      " [0.86033495 0.11550538 0.02415967]\n",
      " [0.10245068 0.8352178  0.06233152]\n",
      " [0.22540499 0.62644005 0.14815497]\n",
      " [0.03223367 0.15185709 0.81590924]\n",
      " [0.02938647 0.12048479 0.85012874]\n",
      " [0.07251345 0.19059062 0.73689594]\n",
      " [0.43454531 0.35671561 0.20873908]\n",
      " [0.37783241 0.50774465 0.11442294]\n",
      " [0.36585256 0.22773207 0.40641537]\n",
      " [0.87176802 0.09413344 0.03409854]\n",
      " [0.02964288 0.1198694  0.85048771]\n",
      " [0.21789693 0.65187661 0.13022645]\n",
      " [0.78154627 0.18777494 0.03067879]\n",
      " [0.14795408 0.10173364 0.75031227]\n",
      " [0.03878409 0.18316342 0.77805249]\n",
      " [0.74354258 0.22324677 0.03321064]\n",
      " [0.12838089 0.82657595 0.04504315]\n",
      " [0.02717296 0.12462072 0.84820632]\n",
      " [0.16029081 0.63758964 0.20211955]\n",
      " [0.77708091 0.18734529 0.0355738 ]\n",
      " [0.05539337 0.14871234 0.79589429]\n",
      " [0.04338837 0.25682539 0.69978624]\n",
      " [0.03172121 0.13910852 0.82917027]\n",
      " [0.0994962  0.42850563 0.47199817]\n",
      " [0.22938073 0.62016663 0.15045264]\n",
      " [0.02679394 0.13169871 0.84150736]\n",
      " [0.14872867 0.68393812 0.1673332 ]\n",
      " [0.48313157 0.25260528 0.26426315]\n",
      " [0.0398311  0.22875769 0.73141121]\n",
      " [0.08351949 0.85809025 0.05839026]\n",
      " [0.02964288 0.12271556 0.84764156]\n",
      " [0.68153273 0.21575791 0.10270936]\n",
      " [0.09929472 0.72783161 0.17287367]\n",
      " [0.22324541 0.67583439 0.1009202 ]\n",
      " [0.11747269 0.67011833 0.21240898]\n",
      " [0.51944055 0.44047359 0.04008586]\n",
      " [0.5269848  0.33823856 0.13477664]\n",
      " [0.06878975 0.39055591 0.54065434]\n",
      " [0.7194733  0.18052644 0.10000026]\n",
      " [0.06484153 0.20466637 0.73049211]\n",
      " [0.02640254 0.21064119 0.76295627]\n",
      " [0.15617647 0.14324678 0.70057675]\n",
      " [0.17610687 0.48924114 0.33465199]\n",
      " [0.13695888 0.58537858 0.27766255]\n",
      " [0.69538461 0.1661814  0.13843399]\n",
      " [0.59620398 0.33074725 0.07304877]\n",
      " [0.04137541 0.16342536 0.79519923]\n",
      " [0.04838748 0.71611398 0.23549855]\n",
      " [0.8132861  0.15312827 0.03358563]\n",
      " [0.69790167 0.09138686 0.21071148]\n",
      " [0.04244325 0.24107627 0.71648048]\n",
      " [0.16954336 0.60078679 0.22966985]\n",
      " [0.35968017 0.28213201 0.35818782]\n",
      " [0.30603848 0.59216401 0.1017975 ]\n",
      " [0.5276201  0.43723124 0.03514866]\n",
      " [0.58332634 0.29636515 0.12030851]\n",
      " [0.34729504 0.61315374 0.03955122]\n",
      " [0.08676339 0.86189488 0.05134173]\n",
      " [0.07771017 0.14531114 0.77697869]\n",
      " [0.30829709 0.608721   0.0829819 ]\n",
      " [0.22829155 0.34365639 0.42805206]\n",
      " [0.04620529 0.11894174 0.83485297]\n",
      " [0.46726142 0.13382095 0.39891763]\n",
      " [0.30587806 0.37787525 0.31624669]\n",
      " [0.0854103  0.18716156 0.72742814]\n",
      " [0.07918207 0.53419549 0.38662244]\n",
      " [0.02776829 0.12553318 0.84669853]\n",
      " [0.57227723 0.34231892 0.08540384]\n",
      " [0.22066193 0.61908602 0.16025206]\n",
      " [0.58778918 0.33170426 0.08050656]\n",
      " [0.09745382 0.6211518  0.28139437]\n",
      " [0.70431781 0.21915809 0.0765241 ]\n",
      " [0.78297964 0.17463941 0.04238095]\n",
      " [0.14899728 0.69287618 0.15812654]\n",
      " [0.03983713 0.13393239 0.82623048]\n",
      " [0.62154926 0.23832144 0.1401293 ]\n",
      " [0.05164895 0.16896725 0.77938381]\n",
      " [0.74636061 0.20517471 0.04846467]\n",
      " [0.10354991 0.60360993 0.29284016]\n",
      " [0.4024193  0.48927058 0.10831012]\n",
      " [0.60482246 0.31815452 0.07702303]\n",
      " [0.07933199 0.85903137 0.06163664]\n",
      " [0.21021053 0.58710048 0.20268899]\n",
      " [0.08158052 0.78204372 0.13637576]\n",
      " [0.6089692  0.23766653 0.15336427]\n",
      " [0.27033183 0.14451478 0.58515339]\n",
      " [0.17058242 0.1571924  0.67222518]\n",
      " [0.63168434 0.20567223 0.16264342]\n",
      " [0.0395285  0.24360314 0.71686836]\n",
      " [0.04562286 0.33411109 0.62026604]\n",
      " [0.36351332 0.35128438 0.2852023 ]\n",
      " [0.49587649 0.27169864 0.23242487]\n",
      " [0.04766729 0.15395961 0.7983731 ]\n",
      " [0.02865863 0.14961893 0.82172244]\n",
      " [0.21121773 0.69717691 0.09160536]\n",
      " [0.07137157 0.27489941 0.65372903]\n",
      " [0.03045078 0.13387452 0.83567469]\n",
      " [0.03606569 0.12352654 0.84040777]\n",
      " [0.656982   0.20228924 0.14072876]\n",
      " [0.86898315 0.10286564 0.02815121]\n",
      " [0.22424109 0.2917442  0.48401471]\n",
      " [0.13585615 0.49627373 0.36787012]\n",
      " [0.36116389 0.3677556  0.27108051]\n",
      " [0.03255827 0.15356739 0.81387433]\n",
      " [0.0840084  0.63772183 0.27826977]\n",
      " [0.04888181 0.15600763 0.79511056]\n",
      " [0.1148777  0.66210392 0.22301838]\n",
      " [0.02625699 0.12550279 0.84824022]\n",
      " [0.02713007 0.11655336 0.85631657]\n",
      " [0.33392169 0.2952389  0.37083941]]\n"
     ]
    }
   ],
   "source": [
    "c1 = DecisionTreeClassifier()\n",
    "c2 = CustomEstimator(n_estimators=50,base_estimator=c1,verbose=1,n_jobs=2)\n",
    "c3 = RandomForestClassifier(n_estimators=50, max_leaf_nodes=16, n_jobs=-1)\n",
    "\n",
    "c1.fit(X_train,y_train)\n",
    "c2.fit(X_train,y_train)\n",
    "c3.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print(c1.predict_proba(X_test))\n",
    "print(c2.predict_proba(X_test))\n",
    "print(c3.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e92a102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbf/rej : 0.885\n",
      "rbf/raC : 0.79\n",
      "Lap/rej : 0.9\n",
      "Lap/raC : 0.79\n"
     ]
    }
   ],
   "source": [
    "#Accuracy comparison w/different parameters\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf1 = CustomEstimator(n_estimators=50,base_estimator=DecisionTreeClassifier())\n",
    "clf2 = CustomEstimator(n_estimators=100,base_estimator=DecisionTreeClassifier(),bootstrap_method=\"randomchoice\")\n",
    "clf3 = CustomEstimator(n_estimators=50,base_estimator=DecisionTreeClassifier(),similarity_metric=\"laplacian\")\n",
    "clf4 = CustomEstimator(n_estimators=100,base_estimator=DecisionTreeClassifier(),similarity_metric=\"laplacian\",bootstrap_method=\"randomchoice\")\n",
    "\n",
    "estimators = [(\"rbf/rej\",clf1),(\"rbf/raC\",clf2),(\"Lap/rej\",clf3),(\"Lap/raC\",clf4)]\n",
    "\n",
    "for i in range(0,len(estimators)):\n",
    "    estimators[i][1].fit(X_train, y_train)\n",
    "    y_pred = estimators[i][1].predict(X_test)\n",
    "    print(estimators[i][0],\":\",metrics.accuracy_score(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86be773b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores:\n",
      "RandomForestClassifier : 0.815\n",
      "SVC : 0.865\n",
      "DecisionTreeClassifier : 0.81\n",
      "BaggingClassifier : 0.83\n",
      "CustomEstimator : 0.835\n",
      "CustomEstimator : 0.865\n",
      "CustomEstimator : 0.805\n"
     ]
    }
   ],
   "source": [
    "# Comparative tests\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=50, max_leaf_nodes=16, n_jobs=-1)\n",
    "\n",
    "svc_clf = SVC(probability=True)\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=16)\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, max_samples=100, bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "custom_clf = CustomEstimator(n_estimators=50,base_estimator=DecisionTreeClassifier())\n",
    "\n",
    "custom_clf2 = CustomEstimator(n_estimators=50,similarity_metric=\"laplacian\")\n",
    "\n",
    "custom_clf3 = CustomEstimator(n_estimators=50,similarity_metric=\"laplacian\", bootstrap_method=\"randomchoice\")\n",
    "\n",
    "print(\"Accuracy scores:\")\n",
    "for clf in (rnd_clf, svc_clf, tree_clf, bag_clf, custom_clf, custom_clf2, custom_clf3):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,\":\",metrics.accuracy_score(y_test, y_pred))\n",
    "    \n",
    "#     results, names = list(), list()\n",
    "#     scores = evaluate_model(clf, X, y)\n",
    "#     results.append(scores)\n",
    "#     names.append(clf.__class__.__name__)\n",
    "#     print('%s %.3f (%.3f)' % (\"Cross validation score:\", np.mean(scores), np.std(scores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7692dd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.36713288, -0.49438263, ...,  0.93817176,\n",
       "         0.26485983,  0.72455268],\n",
       "       [ 0.36713288,  1.        , -0.70683046, ...,  0.46766292,\n",
       "         0.65104316, -0.10009633],\n",
       "       [-0.49438263, -0.70683046,  1.        , ..., -0.35923055,\n",
       "        -0.43796732,  0.0947128 ],\n",
       "       ...,\n",
       "       [ 0.93817176,  0.46766292, -0.35923055, ...,  1.        ,\n",
       "         0.211382  ,  0.79763951],\n",
       "       [ 0.26485983,  0.65104316, -0.43796732, ...,  0.211382  ,\n",
       "         1.        , -0.36418777],\n",
       "       [ 0.72455268, -0.10009633,  0.0947128 , ...,  0.79763951,\n",
       "        -0.36418777,  1.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import check_pairwise_arrays\n",
    "\n",
    "pairwise_kernels(X, Y=None, metric=\"cosine\")\n",
    "\n",
    "#[‘additive_chi2’, ‘chi2’, ‘linear’, ‘poly’, ‘polynomial’, ‘rbf’, ‘laplacian’, ‘sigmoid’, ‘cosine’]\n",
    "\n",
    "#normalized metrics: rbf, laplacian\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87d1b7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 447 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.8825171912593538, 0.8825171912593538, 0.8825171912593538]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "\n",
    "\n",
    "a = np.random.rand(3,2)\n",
    "\n",
    "%time np.sum(a[0],axis=0)\n",
    "\n",
    "%time [Parallel(n_jobs=8)(delayed(np.sum)(a[0],axis=0) for i in range(len(a)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38656f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
